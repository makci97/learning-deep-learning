{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Traslation with Attention\n",
    "Disclaimer: This notebook is an adopted version of [this repository](https://github.com/keon/seq2seq) and [this tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).\n",
    "\n",
    "In this tutorial we'll learn how to build recurrent neural network with attention mechanism to automatically translate from German to English! \n",
    "\n",
    "<img src=\"static/model.png\" width=300 align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "# for text processing\n",
    "import spacy\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "batch_size = 32\n",
    "hidden_size = 512\n",
    "embed_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "1. To train NMT model in supervised manner we need dataset of parallel texts for 2 (or more) languages. Today we'll use **Multi30k** dataset from `torchtext` package. It's a small dataset containing exactly what we need - parallel sentences in German and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchtext.datasets.Multi30k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. To tokenize text we'll use `spacy`. Before using it you have to download German and English language packages:\n",
    "```bash\n",
    "python3 -m spacy download de\n",
    "python3 -m spacy download en\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To operate with text we'll use handy `torchtext` abstractions: [Field](https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Field) and [BucketIterator](https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator). Read the docs on these classes for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all data loading and preparation stuff in one function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(batch_size):\n",
    "    spacy_de = spacy.load('de')\n",
    "    spacy_en = spacy.load('en')\n",
    "    url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "    def tokenize_de(text):\n",
    "        return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "    DE = torchtext.data.Field(tokenize=tokenize_de, include_lengths=True, init_token='<sos>', eos_token='<eos>')\n",
    "    EN = torchtext.data.Field(tokenize=tokenize_en, include_lengths=True, init_token='<sos>', eos_token='<eos>')\n",
    "    \n",
    "    train, val, test = torchtext.datasets.Multi30k.splits(exts=('.de', '.en'), fields=(DE, EN))\n",
    "    DE.build_vocab(train.src, min_freq=2)\n",
    "    EN.build_vocab(train.trg, max_size=10000)\n",
    "    train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "            (train, val, test), batch_size=batch_size, repeat=False)\n",
    "    return train_iter, val_iter, test_iter, DE, EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter, DE, EN = load_dataset(batch_size)\n",
    "de_size, en_size = len(DE.vocab), len(EN.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab one batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`batch` has 2 attributes: `src` and `trg`. Each contains tuple with numerical representation of the sentences with similar lengths (padded) and their original lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"input batch shape\", batch.src[0].shape)\n",
    "print(\"input batch lengths\", batch.src[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `DE` and `EN` you can convert from string representation to numerical and back (`.stoi` and `.itos` methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"encoded sentence\", batch.src[0][:, 0])\n",
    "print(\"encoded sentence\", [DE.vocab.itos[token] for token in batch.src[0][:, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have data, it's time build models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (2 points). Seq2Seq\n",
    "A **Sequence to Sequence** network, or **seq2seq** network, or **Encoder-Decoder** network, is a model consisting of two RNNs called the encoder and decoder. The encoder reads an input sequence and outputs a single vector, and the decoder reads that vector to produce an output sequence. Unlike sequence prediction with a single RNN, where every input corresponds to an output, the seq2seq model frees us from sequence length and order, which makes it ideal for translation between two languages.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the ideal case, encodes the “meaning” of the input sequence into a single vector — a single point in some N dimensional space of sentences.\n",
    "\n",
    "<img src=\"static/seq2seq.png\" width=1000 align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The encoder of a seq2seq network is a bidirectional GRU RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word. To keep hidden size of fixed shape we sum outputs over 2 directions.\n",
    "\n",
    "**Note:** here we use [nn.GRU](https://pytorch.org/docs/stable/nn.html#torch.nn.GRU), not [nn.GRUCell](https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell). Read the docs to understand the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size,\n",
    "                 n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.embed = nn.Embedding(input_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, src, hidden=None):\n",
    "        \"\"\"Encodes input sequence\n",
    "\n",
    "        Args:\n",
    "            src (torch tensor of shape (t, b)): input sequence\n",
    "            hidden (torch tensor of shape (n_layers * n_directions, b, h)): prev hidden state (can be None)\n",
    "        \n",
    "        Returns:\n",
    "            outputs (torch tensor of shape (t, b, h)): encoded sequence (dicrections are summed)\n",
    "            hidden (torch tensor of shape (n_layers * n_directions, b, h)): hidden state\n",
    "        \"\"\"\n",
    "        embedded = ## embed input\n",
    "        outputs, hidden = ## forward recurrent unit\n",
    "        \n",
    "        # sum bidirectional outputs\n",
    "        outputs = outputs.view(outputs.shape[0], outputs.shape[1], 2, self.hidden_size)\n",
    "        outputs = outputs.sum(dim=2)\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "In the simplest seq2seq decoder uses only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder. At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder’s last hidden state).\n",
    "    \n",
    "But in this tutorial we'll pump our decoder with attention mechanism!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to “focus” on a different part of the encoder’s outputs for every step of the decoder’s own outputs. First we calculate a set of attention weights. These will be multiplied by the encoder output vectors to create a weighted combination. The result should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n",
    "\n",
    "<img src=\"static/attention.png\" width=500 align=\"center\"/>\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward layer, using the decoder’s input and hidden state as inputs. Below you can find short description of the so-called Bahdanau attention mechanism (details can be found in the paper [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)).\n",
    "\n",
    "Attention weights ($\\alpha_{ij}$):\n",
    "\n",
    "$$\n",
    "    e_{ij} = f(s_{i-1}, h_j) = v \\tanh(W [s_{i-1}, h_j] + b) \\\\\n",
    "    \\alpha_{ij} = softmax(e_{ij}) = \\frac{\\exp{e_{ij}}}{\\sum_j{\\exp{e_{ij}}}}\n",
    "$$\n",
    "\n",
    "Here $s_{i-1}$ - hidden state of decoder (`hidden`), $h_j$ - hidden state of encoder (`encoder_outputs`), $v$, $W$, $b$ - learnable parameters.\n",
    "\n",
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        \n",
    "        # setup attention parameters\n",
    "        self.v = nn.Parameter(torch.zeros(hidden_size))\n",
    "        \n",
    "        stdv = 1. / np.sqrt(self.v.shape[0])\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"Calculates attention weights\n",
    "\n",
    "        Args:\n",
    "            hidden (torch tensor of shape (b, h)): prev hidden state (can be None)\n",
    "            encoder_outputs (torch tensor of shape (t, b, h)): encoded sequence\n",
    "        \n",
    "        Returns:\n",
    "            attn_weights (torch tensor of shape (b, 1, t)): attention weights\n",
    "        \"\"\" \n",
    "        \n",
    "        timestep = encoder_outputs.shape[0]\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)  # [B*T*H]\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "        \n",
    "        # [B*T*2H]->[B*T*H]\n",
    "        energy = ## concat h and encoder_outputs, feed to self.attn and then to softmax \n",
    "        energy = energy.transpose(1, 2)  # [B*H*T]\n",
    "        \n",
    "        v = self.v.repeat(encoder_outputs.shape[0], 1).unsqueeze(1)  # [B*1*H]\n",
    "        attn_weights = ## multiply by v vector to get shape [B*1*T]\n",
    "        \n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's insert attention mechanism to decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, output_size,\n",
    "                 n_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embed = nn.Embedding(output_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size + embed_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input, last_hidden, encoder_outputs):\n",
    "        \"\"\"Decodes with attention token by token\n",
    "\n",
    "        Args:\n",
    "            input (torch tensor of shape (b,)): input token\n",
    "            last_hidden (torch tensor of shape (1, b, h)): last hidden\n",
    "            encoder_outputs (torch tensor of shape (t, b, h)): encoded sequence\n",
    "        \n",
    "        Returns:\n",
    "            output (torch tensor of shape (b, vocab_size)): ouput token distribution\n",
    "            hidden (torch tensor of shape (1, b, h)): hidden state\n",
    "            attn_weights (torch tensor of shape (b, 1, t)): attention weights\n",
    "        \"\"\"\n",
    "        # get the embedding of the current input word (last output word)\n",
    "        embedded = self.embed(input).unsqueeze(0)  # (1,B,N)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attention(last_hidden[-1], encoder_outputs)  # (B,1,T)\n",
    "        context = ## apply attention weights to encoder_outputs to get shape # (B,1,N) (don't forget to transpose encoder_outputs)\n",
    "        context = context.transpose(0, 1)  # (1,B,N)\n",
    "        \n",
    "        # combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat([embedded, context], 2)\n",
    "        output, hidden = ## forward recurrent unit \n",
    "        output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
    "        \n",
    "        context = context.squeeze(0)\n",
    "        output = self.out(torch.cat([output, context], 1))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap in a single Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, sos_token, max_len):\n",
    "        \"\"\"Sequence-to-sequence inference\n",
    "\n",
    "        Args:\n",
    "            src (torch tensor of shape (t, b)): input sequence\n",
    "        \n",
    "        Returns:\n",
    "            outputs (torch tensor of shape (b, vocab_size)): ouput token distribution\n",
    "        \"\"\"\n",
    "        device = src.device\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        vocab_size = self.decoder.output_size\n",
    "        outputs = torch.zeros(max_len, batch_size, vocab_size).to(device)\n",
    "\n",
    "        encoder_output, hidden = self.encoder(src)\n",
    "        hidden = hidden[:self.decoder.n_layers]\n",
    "        \n",
    "        output = torch.full((batch_size,), sos_token, dtype=torch.long).to(device)\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = ## apply decoder\n",
    "            outputs[t] = output\n",
    "            \n",
    "            top1 = output.data.max(1)[1]\n",
    "            output = top1.to(device)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (1 point). Train-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.0001\n",
    "grad_clip = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(de_size, embed_size, hidden_size, n_layers=2, dropout=0.5)\n",
    "decoder = Decoder(embed_size, hidden_size, en_size, n_layers=1, dropout=0.0)\n",
    "seq2seq = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "optimizer = optim.Adam(seq2seq.parameters(), lr=lr)\n",
    "print(seq2seq)\n",
    "\n",
    "trg_sos_token = EN.vocab.stoi['<sos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapped `train` and `evaluate` ops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter, vocab_size, device, DE, EN):\n",
    "    model.eval()\n",
    "    pad = EN.vocab.stoi['<pad>']\n",
    "    total_loss = 0\n",
    "    for b, batch in enumerate(val_iter):\n",
    "        src, len_src = batch.src\n",
    "        trg, len_trg = batch.trg\n",
    "\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # apply model\n",
    "            output = ## your code here\n",
    "            \n",
    "            # calculate nll loss\n",
    "            # 1) don't take into account first token (it's always <sos>)\n",
    "            # 2) don't take into account pad token (ignore_index argument)\n",
    "            loss = ## your code here\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_iter)\n",
    "\n",
    "\n",
    "def train(e, model, optimizer, train_iter, vocab_size, device, grad_clip, DE, EN):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pad = EN.vocab.stoi['<pad>']\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        src, len_src = batch.src\n",
    "        trg, len_trg = batch.trg\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # apply model\n",
    "        output = ## your code here\n",
    "        \n",
    "        # calculate nll loss\n",
    "        # 1) don't take into account first token (it's always <sos>)\n",
    "        # 2) don't take into account pad token (ignore_index argument)\n",
    "        loss = ## your code here\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # clip gradients using nn.utils.clip_grad_norm_ by `grad_clip` value\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if b % 10 == 0 and b != 0:\n",
    "            total_loss = total_loss / 10\n",
    "            print(\"[%d][loss:%5.2f][pp:%5.2f]\" % (b, total_loss, np.exp(total_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor how our training is going on, let's translate fixed batch of sentences every epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_test_batch = next(iter(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_translations(seq2seq, batch, device, trg_sos_token, max_len=10, max_examples=5):\n",
    "    sentence_encoded = batch.src[0].to(device)\n",
    "    \n",
    "    for example_i in range(min(batch.src[0].shape[1], max_examples)):\n",
    "        input_encoded = batch.src[0][:, example_i]\n",
    "        input = \" \".join([DE.vocab.itos[index] for index in input_encoded][1:batch.src[1][example_i]])\n",
    "        \n",
    "        result_encoded = seq2seq(sentence_encoded, trg_sos_token, max_len)\n",
    "        result_encoded = result_encoded.argmax(dim=2)[:, example_i]\n",
    "        pred = \" \".join([EN.vocab.itos[index] for index in result_encoded][1:])\n",
    "\n",
    "        gt_encoded = batch.trg[0][:, example_i]\n",
    "        gt = \" \".join([EN.vocab.itos[index] for index in gt_encoded][1:batch.trg[1][example_i]])\n",
    "        \n",
    "        print(\"input:\\t\", input)\n",
    "        print(\"pred:\\t\", pred)\n",
    "        print(\"gt:\\t\", gt)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run train-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = None\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    train(e, seq2seq, optimizer, train_iter, en_size, device, grad_clip, DE, EN)\n",
    "    val_loss = evaluate(seq2seq, val_iter, en_size, device, DE, EN)\n",
    "    print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\" % (e, val_loss, np.exp(val_loss)))\n",
    "\n",
    "    # save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        print(\"[!] saving model...\")\n",
    "        if not os.path.isdir(\"weights\"):\n",
    "            os.makedirs(\"weights\")\n",
    "        torch.save(seq2seq.state_dict(), 'weights/seq2seq_%d.pth' % (e))\n",
    "        best_val_loss = val_loss\n",
    "    \n",
    "    print(\"Samples from test:\")\n",
    "    show_translations(seq2seq, fixed_test_batch, device, trg_sos_token, max_len=20, max_examples=5)\n",
    "    print()\n",
    "        \n",
    "test_loss = evaluate(seq2seq, test_iter, en_size, device, DE, EN)\n",
    "print(\"[TEST] loss:%5.2f\" % test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, we did it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
