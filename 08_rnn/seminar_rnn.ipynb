{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text with RNN\n",
    "Disclaimer: This notebook is an adopted version of [this](https://github.com/yandexdataschool/Practical_DL/blob/fall18/week06_rnn/seminar_pytorch.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time you'll find yourself delving into the heart of recurrent neural networks on a class of toy problems.\n",
    "\n",
    "Struggle to find a name for the variable? Let's see how you'll come up with a name for your son/daughter. Surely no human has expertize over what is a good child name, so let us train RNN instead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The dataset contains ~8k earthling names from different cultures, all in latin transcript. This notebook has been designed so as to allow you to quickly swap names for something similar: deep learning article titles, IKEA furniture, pokemon names, etc.\n",
    "\n",
    "Let's read and explore our data. You can find it at `./data` dir. **Note:** all names in `names.txt` starts with space symbol ` `."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = ' '\n",
    "\n",
    "with open(\"data/names.txt\") as fin:\n",
    "    lines = fin.read()[:-1].split('\\n')\n",
    "    lines = [start_token + line for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"# of samples = \",len(lines))\n",
    "print()\n",
    "\n",
    "for x in lines[::1000]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(map(len, lines))\n",
    "print(\"max length = \", MAX_LENGTH)\n",
    "\n",
    "plt.title('Sequence length distribution')\n",
    "plt.hist(list(map(len, lines)), bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (1 point). Text preprocessing\n",
    "\n",
    "First we need next to collect a \"vocabulary\" of all unique tokens i.e. unique characters. We can then encode inputs as a sequence of character ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all unique characters go here\n",
    "tokens = list(set(''.join(lines)))\n",
    "\n",
    "num_tokens = len(tokens)\n",
    "print ('num_tokens = ', num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert characters to ids\n",
    "\n",
    "Torch is built for crunching numbers, not strings. \n",
    "To train our neural network, we'll need to replace characters with their indices in tokens list. Let's compose a dictionary that does this mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = ## your code here\n",
    "\n",
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
    "\n",
    "for i in range(num_tokens):\n",
    "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
    "\n",
    "print(\"Seems alright!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create function to convert list of **lines** into matrix of **ids**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(lines, max_len=None, pad=token_to_id[' ']):\n",
    "    \"\"\"Casts a list of lines into id-matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, lines))\n",
    "    lines_ix = np.zeros([len(lines), max_len], dtype='int32') + pad\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        ## your code here\n",
    "\n",
    "    return lines_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lines: \")\n",
    "print('\\n'.join(lines[::2000]))\n",
    "print()\n",
    "print(\"Corresponding matrix: \")\n",
    "print(to_matrix(lines[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (1 point). Vanilla RNN\n",
    "\n",
    "We can rewrite RNN as a consecutive application of dense layer to input $x_t$ and previous rnn state $h_t$. This is exactly what we're gonna do now.\n",
    "\n",
    "<img src=\"./static/rnn.png\" width=480>\n",
    "\n",
    "Since we're training a language model, there should also be:\n",
    "* An embedding layer (see [nn.Embedding](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)) that converts character's id $x_t$ to a vector.\n",
    "* An output layer that predicts probabilities of next character from current hidden state $h_{t+1}$\n",
    "\n",
    "Let's implement this model in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNCell(nn.Module):\n",
    "    def __init__(self, num_tokens, embedding_size=16, hidden_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_tokens, embedding_size)\n",
    "        self.rnn_update = nn.Linear(embedding_size + hidden_size, hidden_size)\n",
    "        self.logits = nn.Linear(hidden_size, num_tokens)\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n",
    "        We'll call it repeatedly to produce the whole sequence.\n",
    "        \n",
    "        :param x: batch of character ids\n",
    "        :param h_prev: batch of previous RNN hidden states\n",
    "        \"\"\"\n",
    "        # get vector embedding of x\n",
    "        x_emb = self.embedding(x)\n",
    "        \n",
    "        # compute next hidden state using self.rnn_update\n",
    "        h_next = ## your code here\n",
    "        \n",
    "        # compute logits for next character probs\n",
    "        logits = ## your code here\n",
    "        \n",
    "        return h_next, F.log_softmax(logits, -1)\n",
    "    \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\"Return RNN state before it processes first input (a.k.a. h_0)\"\"\"\n",
    "        return torch.zeros(batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_rnn_cell = CharRNNCell(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've defined a single RNN step, we can apply it in a loop to get predictions on each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_loop(cell, batch_ix):\n",
    "    \"\"\"\n",
    "    Computes log P(next_character) for all time-steps in lines_ix\n",
    "    :param lines_ix: an matrix of shape [batch, time], output of to_matrix(lines)\n",
    "    \"\"\"\n",
    "    batch_size, max_length = batch_ix.size()\n",
    "    hid_state = cell.initial_state(batch_size)\n",
    "    \n",
    "    logprobs = []\n",
    "    for x_t in batch_ix.transpose(0, 1):\n",
    "        hid_state, logp_next = ## your code here\n",
    "        logprobs.append(logp_next)\n",
    "        \n",
    "    return torch.stack(logprobs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ix = to_matrix(lines[:5])\n",
    "batch_ix = torch.LongTensor(batch_ix)\n",
    "\n",
    "logp_seq = rnn_loop(char_rnn_cell, batch_ix)\n",
    "print(logp_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood and gradients\n",
    "\n",
    "We can now train our neural network to minimize crossentropy (maximize log-likelihood) with the actual next tokens.\n",
    "\n",
    "To do so in a vectorized manner, we take `batch_ix[:, 1:]` - a matrix of token ids shifted i step to the left so i-th element is acutally the \"next token\" for i-th prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_logp = logp_seq[:, :-1]\n",
    "actual_next_tokens = batch_ix[:, 1:]\n",
    "\n",
    "# here's a loss magic spell (what does this lines mean?)\n",
    "logp_next = torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:, :, None])\n",
    "loss = -logp_next.mean()\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our char-rnn exactly the same way we train any deep learning model before. The only difference is that this time we sample lines, not images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_rnn_cell = CharRNNCell(num_tokens).to(device)\n",
    "opt = torch.optim.Adam(char_rnn_cell.parameters())\n",
    "history = []\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_ix = to_matrix(random.sample(lines, 32), max_len=MAX_LENGTH)\n",
    "    batch_ix = torch.from_numpy(batch_ix).type(torch.long).to(device)\n",
    "    logp_seq = rnn_loop(char_rnn_cell, batch_ix)\n",
    "    \n",
    "    # compute loss\n",
    "    ## your code here\n",
    "    \n",
    "    # train with backprop\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # dump statistics\n",
    "    history.append(loss.item())\n",
    "    if (i + 1) % 100 == 0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history, label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text\n",
    "Once we've trained our network a bit, let's get to actually generating stuff. We trained a so-called **language model**. Mathematically our neural network models probability of the next character conditioned on previous characters:\n",
    "$$\n",
    "    RNN(w_{t-1}, ..., w_{1}) = P(w_t | w_{t-1}, ..., w_{1})\n",
    "$$\n",
    "\n",
    "If we want to generate new sequence with our language model, we feed some seed charactes to it and then greedily generate new characters. Below, you can find the implementation of the sampling function. Read and **understand** this code. After, answer the question:\n",
    "\n",
    "**Q**: What is the purpose the parameter `temperature`? What happens if we increase/decrease it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(cell, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    '''\n",
    "    The function generates text given a phrase of length `max_length`.\n",
    "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
    "    :param max_length: maximum output length, including seed_phrase\n",
    "    :param temperature: coefficient for sampling\n",
    "    '''\n",
    "    \n",
    "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
    "    x_sequence = torch.tensor([x_sequence]).type(torch.long).to(device)\n",
    "\n",
    "    hid_state = cell.initial_state(batch_size=1)\n",
    "    \n",
    "    # feed the seed phrase, if any\n",
    "    for i in range(len(seed_phrase) - 1):\n",
    "        hid_state, _ = cell(x_sequence[:, i], hid_state)\n",
    "    \n",
    "    # start generating\n",
    "    for _ in range(max_length - len(seed_phrase)):\n",
    "        hid_state, logp_next = cell(x_sequence[:, -1], hid_state)\n",
    "        p_next = F.softmax(logp_next / temperature, dim=-1).detach().cpu().numpy()[0]\n",
    "        \n",
    "        # sample next token and push it back into x_sequence\n",
    "        next_ix = np.random.choice(num_tokens, p=p_next)\n",
    "        next_ix = torch.tensor([[next_ix]]).type(torch.long).to(device)\n",
    "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
    "        \n",
    "    return ''.join([tokens[ix] for ix in x_sequence.detach().cpu().numpy()[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completely random names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sample(char_rnn_cell))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random names starting with seed phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    print(generate_sample(char_rnn_cell, seed_phrase=' Lol'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (2 points). LSTM\n",
    "Before we used vanilla RNN cell to generate text, but there're [a lot of problems with it](https://towardsdatascience.com/lecture-evolution-from-vanilla-rnn-to-gru-lstms-58688f1da83a). Nowadays nobody uses vanilla RNN cells - everybody uses more pumped cells like [LSTM](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell) or [GRU](https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell) cells. Here're some scary (at the first sight) images, explaining how LSTM works:\n",
    "\n",
    "<img src=\"./static/lstm.png\">\n",
    "\n",
    "More details you can find in PyTorch docs: [LSTM](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell) and [GRU](https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** will consist of 2 parts. Each task weights 1 point.\n",
    "\n",
    "1. Implement CharRNN with 2-layered LSTM or GRU cell\n",
    "2. Train your model on **your** text dataset (some ideas can be found below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for datasets:\n",
    "\n",
    "* Novels/poems/songs of your favorite author\n",
    "* News titles/clickbait titles\n",
    "* RAP\n",
    "* Source code of Linux or PyTorch\n",
    "* Molecules in [smiles](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) format\n",
    "* Melody in notes/chords format\n",
    "* Ikea catalog titles\n",
    "* Pokemon names\n",
    "* Cards from Magic, the Gathering / Hearthstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
